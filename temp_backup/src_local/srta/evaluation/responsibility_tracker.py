"""
SRTA Responsibility Tracker Module
è²¬ä»»è¿½è·¡ãƒ»é€æ˜æ€§è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import time
import re
import logging
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ResponsibilityLevel(Enum):
    FULLY_TRACEABLE = "Fully Traceable"
    MOSTLY_TRACEABLE = "Mostly Traceable"
    PARTIALLY_TRACEABLE = "Partially Traceable"
    NOT_TRACEABLE = "Not Traceable"

@dataclass
class ResponsibilityMetrics:
    decision_traceability: float = 0.0
    data_lineage: float = 0.0
    actor_identification: float = 0.0
    process_transparency: float = 0.0
    overall: float = 0.0

@dataclass
class ResponsibilityResult:
    metrics: ResponsibilityMetrics
    level: ResponsibilityLevel
    traceable_components: List[str]
    missing_components: List[str]
    assessment_message: str
    processing_time: float
    
    def __getitem__(self, key: str) -> Any:
        """è¾æ›¸ã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œ"""
        return getattr(self, key)
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§ã®çµæœå–å¾—"""
        return {
            'metrics': {
                'decision_traceability': self.metrics.decision_traceability,
                'data_lineage': self.metrics.data_lineage,
                'actor_identification': self.metrics.actor_identification,
                'process_transparency': self.metrics.process_transparency,
                'overall': self.metrics.overall
            },
            'level': self.level.value,
            'traceable_components': self.traceable_components,
            'missing_components': self.missing_components,
            'assessment_message': self.assessment_message,
            'processing_time': self.processing_time
        }

class ResponsibilityTracker:
    """è²¬ä»»è¿½è·¡è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.responsibility_thresholds = self.config.get('responsibility_thresholds', {
            ResponsibilityLevel.FULLY_TRACEABLE: 0.9,
            ResponsibilityLevel.MOSTLY_TRACEABLE: 0.75,
            ResponsibilityLevel.PARTIALLY_TRACEABLE: 0.6,
            ResponsibilityLevel.NOT_TRACEABLE: 0.4
        })
        logger.info("ğŸ”— SRTA Responsibility Tracker initialized")
    
    def evaluate(self, context: Dict[str, Any]) -> ResponsibilityResult:
        """è²¬ä»»è¿½è·¡ã®è©•ä¾¡å®Ÿè¡Œ"""
        start_time = time.time()
        
        # è²¬ä»»è¿½è·¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        decision_trace = self._evaluate_decision_traceability(context)
        data_lineage = self._evaluate_data_lineage(context)
        actor_id = self._evaluate_actor_identification(context)
        process_trans = self._evaluate_process_transparency(context)
        
        overall = (decision_trace + data_lineage + actor_id + process_trans) / 4
        
        metrics = ResponsibilityMetrics(
            decision_traceability=decision_trace,
            data_lineage=data_lineage,
            actor_identification=actor_id,
            process_transparency=process_trans,
            overall=overall
        )
        
        level = self._determine_responsibility_level(overall)
        traceable = self._identify_traceable_components(context, metrics)
        missing = self._identify_missing_components(context, metrics)
        message = self._generate_responsibility_message(level)
        
        return ResponsibilityResult(
            metrics=metrics,
            level=level,
            traceable_components=traceable,
            missing_components=missing,
            assessment_message=message,
            processing_time=time.time() - start_time
        )
    
    def _evaluate_decision_traceability(self, context: Dict[str, Any]) -> float:
        """æ„æ€æ±ºå®šã®è¿½è·¡å¯èƒ½æ€§è©•ä¾¡"""
        text = context.get('explanation_text', '')
        base_score = 0.5
        
        # æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã®æ˜ç¤º
        if any(word in text.lower() for word in ['decision', 'chose', 'selected', 'determined']):
            base_score += 0.2
        
        # åˆ¤æ–­æ ¹æ‹ ã®æ˜ç¤º
        if any(word in text.lower() for word in ['based on', 'according to', 'criteria', 'threshold']):
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _evaluate_data_lineage(self, context: Dict[str, Any]) -> float:
        """ãƒ‡ãƒ¼ã‚¿ç³»è­œã®è¿½è·¡å¯èƒ½æ€§è©•ä¾¡"""
        text = context.get('explanation_text', '')
        base_score = 0.4
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨€åŠ
        if any(word in text.lower() for word in ['data', 'dataset', 'source', 'input']):
            base_score += 0.3
        
        # å‡¦ç†éç¨‹ã®èª¬æ˜
        if any(word in text.lower() for word in ['processed', 'analyzed', 'transformed']):
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _evaluate_actor_identification(self, context: Dict[str, Any]) -> float:
        """é–¢ä¸è€…ç‰¹å®šã®æ˜ç¢ºæ€§è©•ä¾¡"""
        base_score = 0.6
        
        # ã‚·ã‚¹ãƒ†ãƒ /äººé–“ã®æ˜ç¤º
        if context.get('actor_type') in ['human', 'system', 'hybrid']:
            base_score += 0.2
        
        # è²¬ä»»è€…ã®æ˜ç¤º
        if context.get('responsible_entity'):
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _evaluate_process_transparency(self, context: Dict[str, Any]) -> float:
        """ãƒ—ãƒ­ã‚»ã‚¹é€æ˜æ€§ã®è©•ä¾¡"""
        text = context.get('explanation_text', '')
        base_score = 0.5
        
        # ãƒ—ãƒ­ã‚»ã‚¹èª¬æ˜ã®å­˜åœ¨
        if any(word in text.lower() for word in ['process', 'method', 'algorithm']):
            base_score += 0.2
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã®æ˜ç¤º
        if re.search(r'\d+\.|step|phase|stage', text.lower()):
            base_score += 0.2
        
        return min(base_score, 1.0)
    
    def _determine_responsibility_level(self, score: float) -> ResponsibilityLevel:
        """è²¬ä»»è¿½è·¡ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š"""
        for level in [ResponsibilityLevel.FULLY_TRACEABLE, ResponsibilityLevel.MOSTLY_TRACEABLE, 
                      ResponsibilityLevel.PARTIALLY_TRACEABLE]:
            if score >= self.responsibility_thresholds[level]:
                return level
        return ResponsibilityLevel.NOT_TRACEABLE
    
    def _identify_traceable_components(self, context: Dict[str, Any], metrics: ResponsibilityMetrics) -> List[str]:
        """è¿½è·¡å¯èƒ½ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç‰¹å®š"""
        traceable = []
        if metrics.decision_traceability > 0.7:
            traceable.append("æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹")
        if metrics.data_lineage > 0.7:
            traceable.append("ãƒ‡ãƒ¼ã‚¿ç³»è­œ")
        if metrics.actor_identification > 0.7:
            traceable.append("é–¢ä¸è€…ç‰¹å®š")
        if metrics.process_transparency > 0.7:
            traceable.append("ãƒ—ãƒ­ã‚»ã‚¹é€æ˜æ€§")
        return traceable
    
    def _identify_missing_components(self, context: Dict[str, Any], metrics: ResponsibilityMetrics) -> List[str]:
        """ä¸è¶³ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç‰¹å®š"""
        missing = []
        if metrics.decision_traceability <= 0.7:
            missing.append("æ„æ€æ±ºå®šæ ¹æ‹ ã®æ˜ç¤º")
        if metrics.data_lineage <= 0.7:
            missing.append("ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±")
        if metrics.actor_identification <= 0.7:
            missing.append("è²¬ä»»è€…æƒ…å ±")
        if metrics.process_transparency <= 0.7:
            missing.append("å‡¦ç†ãƒ—ãƒ­ã‚»ã‚¹è©³ç´°")
        return missing
    
    def _generate_responsibility_message(self, level: ResponsibilityLevel) -> str:
        """è²¬ä»»è¿½è·¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""
        messages = {
            ResponsibilityLevel.FULLY_TRACEABLE: "å®Œå…¨ãªè²¬ä»»è¿½è·¡ãŒå¯èƒ½ã§ã€ç›£æŸ»è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™ã€‚",
            ResponsibilityLevel.MOSTLY_TRACEABLE: "æ¦‚ã­è²¬ä»»è¿½è·¡ãŒå¯èƒ½ã§ã™ãŒã€ä¸€éƒ¨æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚",
            ResponsibilityLevel.PARTIALLY_TRACEABLE: "éƒ¨åˆ†çš„ãªè²¬ä»»è¿½è·¡ã®ã¿å¯èƒ½ã§ã€é‡è¦ãªæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚",
            ResponsibilityLevel.NOT_TRACEABLE: "è²¬ä»»è¿½è·¡ãŒå›°é›£ã§ã€é€æ˜æ€§ã®å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™ã€‚"
        }
        return messages[level]

def main():
    """è²¬ä»»è¿½è·¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”— SRTA Responsibility Tracker Module Test")
    tracker = ResponsibilityTracker()
    
    test_context = {
        'explanation_text': """
Decision: Image classified as 'cat' based on CNN analysis.
Data source: ImageNet training dataset, processed through ResNet-50.
Decision criteria: Confidence threshold set at 87%.
Responsible system: AI Classification Module v2.1
        """.strip(),
        'actor_type': 'system',
        'responsible_entity': 'AI Classification Module'
    }
    
    result = tracker.evaluate(test_context)
    
    print(f"\nğŸ“Š Responsibility Analysis:")
    print(f"   Decision Traceability: {result.metrics.decision_traceability:.1%}")
    print(f"   Data Lineage: {result.metrics.data_lineage:.1%}")
    print(f"   Actor Identification: {result.metrics.actor_identification:.1%}")
    print(f"   Process Transparency: {result.metrics.process_transparency:.1%}")
    print(f"   Overall: {result.metrics.overall:.1%}")
    print(f"ğŸ† Responsibility Level: {result.level.value}")
    print(f"âœ… Traceable: {', '.join(result.traceable_components)}")
    print(f"âŒ Missing: {', '.join(result.missing_components)}")
    print(f"ğŸ“ Assessment: {result.assessment_message}")
    print("\nResponsibility Tracker working! âœ…")

if __name__ == "__main__":
    main()
