"""
SRTA Evaluation Layer - Enhanced Quality Assessment Module
æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ æ”¹è‰¯ç‰ˆ: EvaluationResult subscriptable ã‚¨ãƒ©ãƒ¼ä¿®æ­£ + å‹å®‰å…¨æ€§å¼·åŒ–
"""

import time
import re
import logging
from typing import Dict, List, Any, Union
from dataclasses import dataclass
from enum import Enum

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QualityLevel(Enum):
    EXCELLENT = "Excellent"
    GOOD = "Good"
    FAIR = "Fair"
    POOR = "Poor"

@dataclass
class EvaluationMetrics:
    clarity: float = 0.0
    completeness: float = 0.0
    understandability: float = 0.0
    overall: float = 0.0

@dataclass
class EvaluationResult:
    metrics: EvaluationMetrics
    quality_level: QualityLevel
    improvement_suggestions: List[str]
    assessment_message: str
    processing_time: float
    
    def __getitem__(self, key: str) -> Any:
        """EvaluationResult object subscriptable ã‚¨ãƒ©ãƒ¼å¯¾å¿œ"""
        return getattr(self, key)
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§ã®çµæœå–å¾—"""
        return {
            'metrics': {
                'clarity': self.metrics.clarity,
                'completeness': self.metrics.completeness,
                'understandability': self.metrics.understandability,
                'overall': self.metrics.overall
            },
            'quality_level': self.quality_level.value,
            'improvement_suggestions': self.improvement_suggestions,
            'assessment_message': self.assessment_message,
            'processing_time': self.processing_time
        }

class EvaluationError(Exception):
    """è©•ä¾¡å‡¦ç†å°‚ç”¨ã®ä¾‹å¤–ã‚¯ãƒ©ã‚¹"""
    pass

class EvaluationLayer:
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.quality_thresholds = self.config.get('quality_thresholds', {
            QualityLevel.EXCELLENT: 0.9,
            QualityLevel.GOOD: 0.75,
            QualityLevel.FAIR: 0.6,
            QualityLevel.POOR: 0.4
        })
        logger.info("ğŸ” SRTA Evaluation Layer (Enhanced Quality Assessment Module) initialized")
    
    def evaluate_explanation(self, context: Dict[str, Any]) -> EvaluationResult:
        """
        èª¬æ˜æ–‡ã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹ï¼ˆæ—¢å­˜äº’æ›æ€§ç¶­æŒï¼‰
        
        Args:
            context: è©•ä¾¡å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¾æ›¸
                    å¿…é ˆã‚­ãƒ¼: 'explanation_text' (str)
        
        Returns:
            EvaluationResult: è©•ä¾¡çµæœ
            
        Raises:
            EvaluationError: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ãªå ´åˆ
        """
        start_time = time.time()
        
        try:
            # å…¥åŠ›æ¤œè¨¼
            text = self._validate_input(context)
            
            # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æ”¹è‰¯ç‰ˆï¼‰
            clarity = self._evaluate_clarity(text)
            completeness = self._evaluate_completeness(text)
            understandability = self._evaluate_understandability(text)
            overall = (clarity + completeness + understandability) / 3
            
            metrics = EvaluationMetrics(
                clarity=clarity,
                completeness=completeness,
                understandability=understandability,
                overall=overall
            )
            
            quality_level = self._determine_quality_level(overall)
            suggestions = self._generate_suggestions(text, metrics)
            message = self._generate_message(quality_level)
            
            result = EvaluationResult(
                metrics=metrics,
                quality_level=quality_level,
                improvement_suggestions=suggestions,
                assessment_message=message,
                processing_time=time.time() - start_time
            )
            
            logger.info(f"Evaluation completed: {quality_level.value} ({overall:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"Evaluation failed: {str(e)}")
            raise EvaluationError(f"è©•ä¾¡å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}") from e
    
    def _validate_input(self, context: Dict[str, Any]) -> str:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        if not isinstance(context, dict):
            raise EvaluationError("context must be a dictionary")
        
        text = context.get('explanation_text')
        if text is None:
            raise EvaluationError("context must contain 'explanation_text' key")
        
        if not isinstance(text, str):
            raise EvaluationError("explanation_text must be a string")
        
        return text
    
    def _evaluate_clarity(self, text: str) -> float:
        """æ˜ç¢ºæ€§ã®è©•ä¾¡ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æ”¹è‰¯ç‰ˆï¼‰"""
        if not text:
            return 0.0
        base_score = 0.5  # ã‚ˆã‚Šä¿å®ˆçš„ãªåˆæœŸå€¤
        
        # æ§‹é€ çš„è¦ç´ ã®æ¤œå‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if re.search(r'[1-9]\.|\*|-|â€¢', text):
            base_score += 0.2
            
        # é©åˆ‡ãªé•·ã•ã®è©•ä¾¡
        word_count = len(text.split())
        if 30 <= word_count <= 200:
            base_score += 0.15
        elif word_count < 10:
            base_score -= 0.1  # çŸ­ã™ãã‚‹å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            
        return min(max(base_score, 0.0), 1.0)
    
    def _evaluate_completeness(self, text: str) -> float:
        """å®Œå…¨æ€§ã®è©•ä¾¡ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æ”¹è‰¯ç‰ˆï¼‰"""
        if not text:
            return 0.0
        base_score = 0.4  # ã‚ˆã‚Šä¿å®ˆçš„ãªåˆæœŸå€¤
        
        # å¿…é ˆè¦ç´ ã®æ¤œå‡ºï¼ˆã‚ˆã‚Šè©³ç´°ï¼‰
        what_indicators = ['what', 'result', 'decision', 'outcome']
        how_indicators = ['how', 'method', 'process', 'procedure']
        why_indicators = ['why', 'reason', 'because', 'since']
        
        text_lower = text.lower()
        has_what = any(word in text_lower for word in what_indicators)
        has_how = any(word in text_lower for word in how_indicators)
        has_why = any(word in text_lower for word in why_indicators)
        
        element_score = sum([has_what, has_how, has_why]) / 3 * 0.4
        base_score += element_score
        
        return min(max(base_score, 0.0), 1.0)
    
    def _evaluate_understandability(self, text: str) -> float:
        """ç†è§£å®¹æ˜“æ€§ã®è©•ä¾¡ï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯æ”¹è‰¯ç‰ˆï¼‰"""
        if not text:
            return 0.0
        base_score = 0.4  # ã‚ˆã‚Šä¿å®ˆçš„ãªåˆæœŸå€¤
        
        # ä¾‹ã®å­˜åœ¨ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if re.search(r'example|instance|such as|for instance|e\.g\.', text.lower()):
            base_score += 0.2
            
        # æ–‡ã®é•·ã•ã®é©åˆ‡æ€§
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        if sentences:
            avg_length = sum(len(s.split()) for s in sentences) / len(sentences)
            if 5 <= avg_length <= 20:
                base_score += 0.2
            elif avg_length > 30:  # é•·ã™ãã‚‹æ–‡ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
                base_score -= 0.1
                
        return min(max(base_score, 0.0), 1.0)
    
    def _determine_quality_level(self, score: float) -> QualityLevel:
        """å“è³ªãƒ¬ãƒ™ãƒ«ã®æ±ºå®šï¼ˆæ—¢å­˜äº’æ›ï¼‰"""
        for level in [QualityLevel.EXCELLENT, QualityLevel.GOOD, QualityLevel.FAIR]:
            if score >= self.quality_thresholds[level]:
                return level
        return QualityLevel.POOR
    
    def _generate_suggestions(self, text: str, metrics: EvaluationMetrics) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆï¼ˆæ—¢å­˜äº’æ›æ”¹è‰¯ç‰ˆï¼‰"""
        suggestions = []
        threshold = 0.7
        
        if metrics.clarity < threshold:
            suggestions.append("æ§‹é€ åŒ–: ç•ªå·ä»˜ããƒªã‚¹ãƒˆã‚„ç®‡æ¡æ›¸ãã§æƒ…å ±ã‚’æ•´ç†ã—ã¦ãã ã•ã„")
        if metrics.completeness < threshold:
            suggestions.append("ç¶²ç¾…æ€§: 'ä½•ã‚’'ã€'ã©ã®ã‚ˆã†ã«'ã€'ãªãœ'ã®è¦³ç‚¹ã‚’å«ã‚ã¦ãã ã•ã„")
        if metrics.understandability < threshold:
            suggestions.append("ç†è§£ä¿ƒé€²: å…·ä½“ä¾‹ã‚„æ¯”å–©ã‚’ä½¿ã£ã¦æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„")
            
        if not suggestions:
            suggestions.append("å„ªç§€ãªå“è³ªã§ã™ - æ”¹å–„ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“")
            
        return suggestions
    
    def _generate_message(self, quality_level: QualityLevel) -> str:
        """è©•ä¾¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆï¼ˆæ—¢å­˜äº’æ›ï¼‰"""
        messages = {
            QualityLevel.EXCELLENT: "Outstanding explanation quality with comprehensive coverage and clarity.",
            QualityLevel.GOOD: "Good quality explanation that effectively communicates the key information.",
            QualityLevel.FAIR: "Acceptable explanation with room for improvement in clarity and detail.",
            QualityLevel.POOR: "Explanation needs significant improvement to meet quality standards."
        }
        return messages[quality_level]

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°ï¼ˆæ—¢å­˜äº’æ›ï¼‰"""
    print("ğŸ” SRTA Evaluation Layer - Enhanced Quality Assessment Module")
    evaluator = EvaluationLayer()
    
    # æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_context = {
        'explanation_text': """
This image is classified as 'cat' with 87% confidence.

Classification reasoning:
1. Shape pattern analysis detected animal features
2. Edge detection identified ear shapes
3. Texture analysis confirmed fur patterns

For example, the triangular ear shapes are characteristic of felines.
This automated analysis provides reliable classification results.
        """.strip()
    }
    
    try:
        result = evaluator.evaluate_explanation(test_context)
        
        print(f"\nğŸ“Š Quality Assessment Results:")
        print(f"   Clarity: {result.metrics.clarity:.1%}")
        print(f"   Completeness: {result.metrics.completeness:.1%}")
        print(f"   Understandability: {result.metrics.understandability:.1%}")
        print(f"   Overall: {result.metrics.overall:.1%}")
        print(f"ğŸ† Quality Level: {result.quality_level.value}")
        print(f"â±ï¸ Processing Time: {result.processing_time:.3f}s")
        print(f"\nğŸ’¡ Suggestions: {', '.join(result.improvement_suggestions)}")
        print(f"\nğŸ” Assessment: {result.assessment_message}")
        
        # subscriptable ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ”§ Subscriptable Test:")
        print(f"   result['quality_level']: {result['quality_level']}")
        print(f"   result.to_dict(): è¾æ›¸å½¢å¼å¤‰æ›æˆåŠŸ")
        
        print("\nEnhanced Evaluation Layer æ”¹è‰¯ç‰ˆå‹•ä½œç¢ºèª! âœ…")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
