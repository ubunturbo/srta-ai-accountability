#!/usr/bin/env python3
"""
SRTA Performance Benchmark Runner
SRTA ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 

Purpose: ç¶™ç¶šçš„æ€§èƒ½æ¸¬å®šãƒ»CIçµ±åˆãƒ»è«–æ–‡ç”¨å®šé‡ãƒ‡ãƒ¼ã‚¿
Author: ubunturbo (Baptist Pastor & AI Researcher)
License: MIT
"""

import time
import psutil
import json
import csv
import statistics
import threading
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime
import memory_profiler
import cProfile
import pstats
import io

@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""
    name: str
    description: str
    iterations: int
    warmup_iterations: int
    target_function: str
    parameters: Dict[str, Any]
    timeout_seconds: int
    memory_profiling: bool
    cpu_profiling: bool

@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    config_name: str
    avg_execution_time_ms: float
    min_execution_time_ms: float
    max_execution_time_ms: float
    std_execution_time_ms: float
    avg_memory_usage_mb: float
    peak_memory_usage_mb: float
    cpu_usage_percent: float
    throughput_ops_per_sec: float
    success_rate: float
    error_count: int
    timestamp: str
    profiling_data: Optional[Dict[str, Any]] = None

class PerformanceMonitor:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics = []
        self.start_time = None
        
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.start_time = time.time()
        self.metrics = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join()
        
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory_info = psutil.virtual_memory()
                process = psutil.Process()
                process_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                metric = {
                    'timestamp': time.time() - self.start_time,
                    'cpu_percent': cpu_percent,
                    'memory_usage_mb': process_memory,
                    'system_memory_percent': memory_info.percent
                }
                self.metrics.append(metric)
                
            except Exception as e:
                print(f"Monitoring error: {e}")
            
            time.sleep(0.1)  # 100ms interval
    
    def get_summary(self) -> Dict[str, float]:
        """ç›£è¦–ã‚µãƒãƒªãƒ¼å–å¾—"""
        if not self.metrics:
            return {}
        
        cpu_values = [m['cpu_percent'] for m in self.metrics]
        memory_values = [m['memory_usage_mb'] for m in self.metrics]
        
        return {
            'avg_cpu_percent': statistics.mean(cpu_values),
            'max_cpu_percent': max(cpu_values),
            'avg_memory_mb': statistics.mean(memory_values),
            'peak_memory_mb': max(memory_values),
            'monitoring_duration': self.metrics[-1]['timestamp'] if self.metrics else 0
        }

class SRTABenchmarkRunner:
    """SRTA ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, output_dir: str = "benchmarks/results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šç¾¤
        self.benchmark_configs = self._create_benchmark_configs()
        
        # çµæœä¿å­˜
        self.results: List[BenchmarkResult] = []
        
        # æ€§èƒ½ç›£è¦–
        self.monitor = PerformanceMonitor()
        
    def _create_benchmark_configs(self) -> List[BenchmarkConfig]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šç¾¤ä½œæˆ"""
        configs = []
        
        # åŸºæœ¬æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        configs.append(BenchmarkConfig(
            name="trace_engine_basic",
            description="Basic trace engine performance",
            iterations=100,
            warmup_iterations=10,
            target_function="benchmark_trace_engine",
            parameters={"depth": 5, "nodes": 100},
            timeout_seconds=30,
            memory_profiling=True,
            cpu_profiling=True
        ))
        
        # æ„å‘³è§£æãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        configs.append(BenchmarkConfig(
            name="semantic_analysis_performance",
            description="Semantic analysis performance under load",
            iterations=50,
            warmup_iterations=5,
            target_function="benchmark_semantic_analysis",
            parameters={"text_length": 1000, "complexity": "medium"},
            timeout_seconds=60,
            memory_profiling=True,
            cpu_profiling=False
        ))
        
        # è²¬ä»»è¿½è·¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        configs.append(BenchmarkConfig(
            name="responsibility_tracking_scale",
            description="Responsibility tracking scalability",
            iterations=200,
            warmup_iterations=20,
            target_function="benchmark_responsibility_tracking",
            parameters={"entities": 500, "relationships": 1000},
            timeout_seconds=120,
            memory_profiling=True,
            cpu_profiling=True
        ))
        
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        configs.append(BenchmarkConfig(
            name="integrated_system_stress",
            description="Full SRTA system under stress",
            iterations=30,
            warmup_iterations=3,
            target_function="benchmark_integrated_system",
            parameters={"concurrent_requests": 10, "data_size": "large"},
            timeout_seconds=300,
            memory_profiling=True,
            cpu_profiling=True
        ))
        
        return configs
    
    def run_all_benchmarks(self) -> Dict[str, Any]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸƒâ€â™‚ï¸ Starting SRTA Performance Benchmarks")
        print("=" * 50)
        
        benchmark_summary = {
            "start_time": datetime.now().isoformat(),
            "benchmarks": [],
            "overall_performance": {},
            "success_count": 0,
            "failure_count": 0
        }
        
        for config in self.benchmark_configs:
            try:
                print(f"\nğŸ”§ Running benchmark: {config.name}")
                print(f"   Description: {config.description}")
                print(f"   Iterations: {config.iterations}")
                
                result = self.run_single_benchmark(config)
                
                if result:
                    self.results.append(result)
                    benchmark_summary["benchmarks"].append({
                        "name": config.name,
                        "status": "success",
                        "avg_time_ms": result.avg_execution_time_ms,
                        "throughput": result.throughput_ops_per_sec,
                        "memory_mb": result.avg_memory_usage_mb
                    })
                    benchmark_summary["success_count"] += 1
                    
                    # çµæœè¡¨ç¤º
                    print(f"   âœ… Avg Time: {result.avg_execution_time_ms:.2f}ms")
                    print(f"   âš¡ Throughput: {result.throughput_ops_per_sec:.1f} ops/sec")
                    print(f"   ğŸ’¾ Memory: {result.avg_memory_usage_mb:.1f}MB")
                    
                else:
                    benchmark_summary["benchmarks"].append({
                        "name": config.name,
                        "status": "failed"
                    })
                    benchmark_summary["failure_count"] += 1
                    print(f"   âŒ Failed")
                    
            except Exception as e:
                print(f"   ğŸ’¥ Error: {str(e)}")
                benchmark_summary["benchmarks"].append({
                    "name": config.name,
                    "status": "error",
                    "error": str(e)
                })
                benchmark_summary["failure_count"] += 1
        
        # åŒ…æ‹¬çš„æ€§èƒ½ã‚µãƒãƒªãƒ¼
        benchmark_summary["overall_performance"] = self._calculate_overall_performance()
        benchmark_summary["end_time"] = datetime.now().isoformat()
        
        # çµæœä¿å­˜
        self.save_benchmark_results(benchmark_summary)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ“Š Benchmark Summary:")
        print(f"   Total: {len(self.benchmark_configs)}")
        print(f"   Success: {benchmark_summary['success_count']}")
        print(f"   Failed: {benchmark_summary['failure_count']}")
        print(f"   Results: {self.output_dir}")
        
        return benchmark_summary
    
    def run_single_benchmark(self, config: BenchmarkConfig) -> Optional[BenchmarkResult]:
        """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        
        execution_times = []
        memory_usages = []
        error_count = 0
        profiling_data = {}
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(config.warmup_iterations):
            try:
                self._execute_benchmark_function(config.target_function, config.parameters)
            except:
                pass  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        
        # ãƒ¡ã‚¤ãƒ³æ¸¬å®š
        self.monitor.start_monitoring()
        
        # CPUãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹
        if config.cpu_profiling:
            profiler = cProfile.Profile()
            profiler.enable()
        
        for i in range(config.iterations):
            try:
                # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
                if config.memory_profiling:
                    start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
                start_time = time.perf_counter()
                
                # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°å®Ÿè¡Œ
                self._execute_benchmark_function(config.target_function, config.parameters)
                
                end_time = time.perf_counter()
                execution_time = (end_time - start_time) * 1000  # ms
                execution_times.append(execution_time)
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
                if config.memory_profiling:
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    memory_usages.append(end_memory)
                
            except Exception as e:
                error_count += 1
                print(f"   Iteration {i+1} error: {str(e)}")
        
        # CPUãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ‚äº†
        if config.cpu_profiling:
            profiler.disable()
            s = io.StringIO()
            ps = pstats.Stats(profiler, stream=s)
            ps.sort_stats('cumulative')
            ps.print_stats(10)  # ãƒˆãƒƒãƒ—10é–¢æ•°
            profiling_data['cpu_profile'] = s.getvalue()
        
        self.monitor.stop_monitoring()
        monitor_summary = self.monitor.get_summary()
        
        # çµæœçµ±è¨ˆè¨ˆç®—
        if not execution_times:
            return None
        
        avg_execution_time = statistics.mean(execution_times)
        min_execution_time = min(execution_times)
        max_execution_time = max(execution_times)
        std_execution_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0
        
        avg_memory = statistics.mean(memory_usages) if memory_usages else monitor_summary.get('avg_memory_mb', 0)
        peak_memory = max(memory_usages) if memory_usages else monitor_summary.get('peak_memory_mb', 0)
        
        throughput = 1000 / avg_execution_time if avg_execution_time > 0 else 0  # ops/sec
        success_rate = (config.iterations - error_count) / config.iterations
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        if monitor_summary:
            profiling_data.update(monitor_summary)
        
        result = BenchmarkResult(
            config_name=config.name,
            avg_execution_time_ms=avg_execution_time,
            min_execution_time_ms=min_execution_time,
            max_execution_time_ms=max_execution_time,
            std_execution_time_ms=std_execution_time,
            avg_memory_usage_mb=avg_memory,
            peak_memory_usage_mb=peak_memory,
            cpu_usage_percent=monitor_summary.get('avg_cpu_percent', 0),
            throughput_ops_per_sec=throughput,
            success_rate=success_rate,
            error_count=error_count,
            timestamp=datetime.now().isoformat(),
            profiling_data=profiling_data
        )
        
        return result
    
    def _execute_benchmark_function(self, function_name: str, parameters: Dict[str, Any]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°å®Ÿè¡Œï¼ˆPhase 2ã§å®Ÿéš›ã®SRTAé–¢æ•°ã«ç½®ãæ›ãˆï¼‰"""
        
        # ç¾åœ¨ã¯æ¨¡æ“¬å®Ÿè¡Œï¼ˆå®Ÿéš›ã®SRTAå®Ÿè£…ã¨æ¥ç¶šå¾Œã«ç½®ãæ›ãˆï¼‰
        if function_name == "benchmark_trace_engine":
            # æ¨¡æ“¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ³å‡¦ç†
            depth = parameters.get("depth", 5)
            nodes = parameters.get("nodes", 100)
            # è¨ˆç®—é›†ç´„çš„ã‚¿ã‚¹ã‚¯ã®æ¨¡æ“¬
            result = sum(i * j for i in range(depth) for j in range(nodes // depth))
            
        elif function_name == "benchmark_semantic_analysis":
            # æ¨¡æ“¬çš„ãªæ„å‘³è§£æå‡¦ç†
            text_length = parameters.get("text_length", 1000)
            complexity = parameters.get("complexity", "medium")
            # æ–‡å­—åˆ—å‡¦ç†ã®æ¨¡æ“¬
            text = "a" * text_length
            result = len(text.split()) * (2 if complexity == "high" else 1)
            
        elif function_name == "benchmark_responsibility_tracking":
            # æ¨¡æ“¬çš„ãªè²¬ä»»è¿½è·¡å‡¦ç†
            entities = parameters.get("entities", 500)
            relationships = parameters.get("relationships", 1000)
            # ã‚°ãƒ©ãƒ•å‡¦ç†ã®æ¨¡æ“¬
            result = entities * relationships // 100
            
        elif function_name == "benchmark_integrated_system":
            # æ¨¡æ“¬çš„ãªçµ±åˆã‚·ã‚¹ãƒ†ãƒ å‡¦ç†
            requests = parameters.get("concurrent_requests", 10)
            data_size = parameters.get("data_size", "medium")
            # è¤‡åˆå‡¦ç†ã®æ¨¡æ“¬
            size_multiplier = {"small": 1, "medium": 10, "large": 100}.get(data_size, 10)
            result = requests * size_multiplier * 42
            
        else:
            raise ValueError(f"Unknown benchmark function: {function_name}")
        
        # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’æ¨¡æ“¬ã™ã‚‹ãŸã‚ã®å°ã•ãªé…å»¶
        time.sleep(0.001)  # 1ms
        
        return result
    
    def _calculate_overall_performance(self) -> Dict[str, float]:
        """åŒ…æ‹¬çš„æ€§èƒ½æŒ‡æ¨™è¨ˆç®—"""
        if not self.results:
            return {}
        
        all_times = [r.avg_execution_time_ms for r in self.results]
        all_memory = [r.avg_memory_usage_mb for r in self.results]
        all_throughput = [r.throughput_ops_per_sec for r in self.results]
        all_success_rates = [r.success_rate for r in self.results]
        
        return {
            "avg_response_time_ms": statistics.mean(all_times),
            "total_throughput_ops_per_sec": sum(all_throughput),
            "avg_memory_efficiency_mb": statistics.mean(all_memory),
            "overall_success_rate": statistics.mean(all_success_rates),
            "performance_stability": 1.0 - (statistics.stdev(all_times) / statistics.mean(all_times)) if len(all_times) > 1 else 1.0
        }
    
    def save_benchmark_results(self, summary: Dict[str, Any]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœä¿å­˜"""
        
        # 1. ã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = self.output_dir / f"benchmark_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # 2. CSVå½¢å¼ï¼ˆè«–æ–‡ç”¨ï¼‰
        if self.results:
            csv_file = self.output_dir / "benchmark_results.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿é™¤å¤–ï¼ˆCSVç”¨ï¼‰
                results_dict = []
                for result in self.results:
                    result_dict = asdict(result)
                    result_dict.pop('profiling_data', None)
                    results_dict.append(result_dict)
                
                if results_dict:
                    writer = csv.DictWriter(f, fieldnames=results_dict[0].keys())
                    writer.writeheader()
                    writer.writerows(results_dict)
        
        # 3. äººé–“èª­ã¿æ˜“ã„ãƒ¬ãƒãƒ¼ãƒˆ
        self.generate_performance_report(summary)
        
        print(f"ğŸ“„ Benchmark results saved to {self.output_dir}")
    
    def generate_performance_report(self, summary: Dict[str, Any]):
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_file = self.output_dir / "performance_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# SRTA Performance Benchmark Report\n\n")
            f.write(f"**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n\n")
            
            # å…¨ä½“ã‚µãƒãƒªãƒ¼
            f.write("## Executive Summary\n\n")
            overall = summary.get("overall_performance", {})
            f.write(f"- **å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {overall.get('avg_response_time_ms', 0):.2f}ms\n")
            f.write(f"- **ç·ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {overall.get('total_throughput_ops_per_sec', 0):.1f} ops/sec\n")
            f.write(f"- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: {overall.get('avg_memory_efficiency_mb', 0):.1f}MB\n")
            f.write(f"- **æˆåŠŸç‡**: {overall.get('overall_success_rate', 0):.1%}\n")
            f.write(f"- **æ€§èƒ½å®‰å®šæ€§**: {overall.get('performance_stability', 0):.3f}\n\n")
            
            # å€‹åˆ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            f.write("## Individual Benchmark Results\n\n")
            for benchmark in summary.get("benchmarks", []):
                if benchmark.get("status") == "success":
                    f.write(f"### {benchmark['name']}\n\n")
                    f.write(f"- **å¹³å‡å®Ÿè¡Œæ™‚é–“**: {benchmark.get('avg_time_ms', 0):.2f}ms\n")
                    f.write(f"- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: {benchmark.get('throughput', 0):.1f} ops/sec\n")
                    f.write(f"- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {benchmark.get('memory_mb', 0):.1f}MB\n\n")
            
            f.write("## CI/CD Integration\n\n")
            f.write("ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¯ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§è‡ªå‹•å®Ÿè¡Œã•ã‚Œã€\n")
            f.write("æ€§èƒ½é€€åŒ–ã®æ—©æœŸæ¤œå‡ºã¨IEEEè«–æ–‡ç”¨ãƒ‡ãƒ¼ã‚¿åé›†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã€‚\n")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ SRTA Benchmark Runner")
    print("Performance measurement for IEEE paper submission")
    print("=" * 55)
    
    runner = SRTABenchmarkRunner()
    
    try:
        summary = runner.run_all_benchmarks()
        
        print("\nğŸ† Benchmark completed successfully!")
        print("ğŸ“Š Ready for continuous performance monitoring!")
        return 0
        
    except Exception as e:
        print(f"âŒ Benchmark execution failed: {str(e)}")
        return 1

if __name__ == "__main__":
    exit(main())