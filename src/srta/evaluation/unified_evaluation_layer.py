"""
Unified SRTA Evaluation System - æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±åˆç‰ˆ
è²¬ä»»è¿½è·¡ + å“è³ªè©•ä¾¡ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ  (æ—¢å­˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é©åˆ)
"""

import time
import logging
from typing import Dict, List, Any
from dataclasses import dataclass, asdict
from datetime import datetime

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã®äº’æ›æ€§
from .evaluation_layer_enhanced import EvaluationLayer, EvaluationResult
from .responsibility_tracker import ResponsibilityTracker, ResponsibilityResult

logger = logging.getLogger(__name__)

@dataclass
class UnifiedEvaluationResult:
    quality_assessment: Dict[str, Any]
    responsibility_analysis: Dict[str, Any]
    correlation_analysis: Dict[str, Any]
    unified_score: float
    recommendations: List[str]
    overall_assessment: str
    timestamp: str
    processing_time: float
    
    def __getitem__(self, key: str) -> Any:
        """è¾æ›¸ã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œ"""
        return getattr(self, key)
    
    def to_dict(self) -> Dict[str, Any]:
        """å®Œå…¨ãªè¾æ›¸å½¢å¼ã§ã®çµæœå–å¾—"""
        return asdict(self)

class UnifiedSRTAEvaluationLayer:
    """SRTAè²¬ä»»è¿½è·¡ + å“è³ªè©•ä¾¡çµ±åˆã‚·ã‚¹ãƒ†ãƒ  (æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé©åˆç‰ˆ)"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        self.quality_evaluator = EvaluationLayer(config)
        self.responsibility_tracker = ResponsibilityTracker(config)
        
        # çµ±åˆã‚¹ã‚³ã‚¢é‡ã¿è¨­å®š
        self.weights = self.config.get('weights', {
            'responsibility': 0.6,  # è²¬ä»»è¿½è·¡ã®é‡è¦åº¦
            'quality': 0.4          # å“è³ªè©•ä¾¡ã®é‡è¦åº¦
        })
        
        logger.info("ğŸ¯ Unified SRTA Evaluation Layer (Project Integration Version) initialized")
    
    def comprehensive_evaluate(self, context: Dict[str, Any]) -> UnifiedEvaluationResult:
        """åŒ…æ‹¬çš„è©•ä¾¡ã®å®Ÿè¡Œ (æ—¢å­˜APIäº’æ›)"""
        start_time = time.time()
        
        try:
            # å€‹åˆ¥è©•ä¾¡ã®å®Ÿè¡Œ
            quality_result = self.quality_evaluator.evaluate_explanation(context)
            responsibility_result = self.responsibility_tracker.evaluate(context)
            
            # ç›¸é–¢åˆ†æ
            correlation_analysis = self._analyze_correlations(responsibility_result, quality_result)
            
            # çµ±åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            unified_score = self._calculate_unified_score(responsibility_result, quality_result)
            
            # çµ±åˆæ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_unified_recommendations(
                responsibility_result, quality_result, correlation_analysis
            )
            
            # ç·åˆè©•ä¾¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            overall_assessment = self._generate_overall_assessment(unified_score, correlation_analysis)
            
            return UnifiedEvaluationResult(
                quality_assessment=quality_result.to_dict(),
                responsibility_analysis=responsibility_result.to_dict(),
                correlation_analysis=correlation_analysis,
                unified_score=unified_score,
                recommendations=recommendations,
                overall_assessment=overall_assessment,
                timestamp=datetime.now().isoformat(),
                processing_time=time.time() - start_time
            )
            
        except Exception as e:
            logger.error(f"Unified evaluation failed: {str(e)}")
            raise
    
    def evaluate_explanation(self, context: Dict[str, Any]) -> UnifiedEvaluationResult:
        """æ—¢å­˜APIäº’æ›ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹"""
        return self.comprehensive_evaluate(context)
    
    def _analyze_correlations(self, resp_result: ResponsibilityResult, qual_result: EvaluationResult) -> Dict[str, Any]:
        """è²¬ä»»è¿½è·¡ã¨å“è³ªè©•ä¾¡ã®ç›¸é–¢åˆ†æ"""
        resp_overall = resp_result.metrics.overall
        qual_overall = qual_result.metrics.overall
        
        # ç›¸é–¢ã‚¹ã‚³ã‚¢è¨ˆç®—
        correlation_score = 1 - abs(resp_overall - qual_overall)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        if resp_overall > 0.8 and qual_overall > 0.8:
            pattern = "é«˜è²¬ä»»è¿½è·¡ãƒ»é«˜å“è³ª"
            insight = "ç†æƒ³çš„ãªçŠ¶æ…‹ - é€æ˜æ€§ã¨å“è³ªã®ä¸¡æ–¹ãŒå„ªç§€"
        elif resp_overall > 0.7 and qual_overall < 0.6:
            pattern = "é«˜è²¬ä»»è¿½è·¡ãƒ»ä½å“è³ª"
            insight = "ãƒ—ãƒ­ã‚»ã‚¹ã¯é€æ˜ã ãŒèª¬æ˜å“è³ªã«èª²é¡Œ"
        elif resp_overall < 0.6 and qual_overall > 0.7:
            pattern = "ä½è²¬ä»»è¿½è·¡ãƒ»é«˜å“è³ª"
            insight = "èª¬æ˜å“è³ªã¯è‰¯ã„ãŒé€æ˜æ€§ã«èª²é¡Œ"
        else:
            pattern = "è¦æ”¹å–„"
            insight = "è²¬ä»»è¿½è·¡ã¨å“è³ªã®ä¸¡æ–¹ã«æ”¹å–„ãŒå¿…è¦"
        
        return {
            'correlation_score': correlation_score,
            'pattern': pattern,
            'insight': insight,
            'responsibility_score': resp_overall,
            'quality_score': qual_overall
        }
    
    def _calculate_unified_score(self, resp_result: ResponsibilityResult, qual_result: EvaluationResult) -> float:
        """çµ±åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        resp_score = resp_result.metrics.overall
        qual_score = qual_result.metrics.overall
        
        unified = (resp_score * self.weights['responsibility'] + 
                  qual_score * self.weights['quality'])
        
        return round(unified, 3)
    
    def _generate_unified_recommendations(self, resp_result: ResponsibilityResult, 
                                        qual_result: EvaluationResult, 
                                        correlation: Dict[str, Any]) -> List[str]:
        """çµ±åˆæ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # å€‹åˆ¥æ¨å¥¨äº‹é …ã®çµ±åˆ
        if resp_result.missing_components:
            recommendations.append(f"è²¬ä»»è¿½è·¡æ”¹å–„: {', '.join(resp_result.missing_components)}")
        
        if qual_result.improvement_suggestions and qual_result.improvement_suggestions[0] != "å„ªç§€ãªå“è³ªã§ã™ - æ”¹å–„ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“":
            recommendations.append(f"å“è³ªæ”¹å–„: {', '.join(qual_result.improvement_suggestions)}")
        
        # ç›¸é–¢ã«åŸºã¥ãæ¨å¥¨äº‹é …
        if correlation['pattern'] == "é«˜è²¬ä»»è¿½è·¡ãƒ»ä½å“è³ª":
            recommendations.append("ãƒ—ãƒ­ã‚»ã‚¹é€æ˜æ€§ã‚’æ´»ã‹ã—ã€èª¬æ˜ã®æ§‹é€ åŒ–ã¨å…·ä½“ä¾‹è¿½åŠ ã‚’é‡ç‚¹å®Ÿæ–½")
        elif correlation['pattern'] == "ä½è²¬ä»»è¿½è·¡ãƒ»é«˜å“è³ª":
            recommendations.append("èª¬æ˜å“è³ªã®é«˜ã•ã‚’ç¶­æŒã—ã¤ã¤ã€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã®æ˜ç¤ºã‚’å¼·åŒ–")
        elif correlation['correlation_score'] < 0.5:
            recommendations.append("è²¬ä»»è¿½è·¡ã¨å“è³ªè©•ä¾¡ã®ä¸€è²«æ€§å‘ä¸ŠãŒå¿…è¦")
        
        return recommendations or ["ç¾çŠ¶ç¶­æŒ - å„ªç§€ãªçŠ¶æ…‹ã‚’ç¶™ç¶š"]
    
    def _generate_overall_assessment(self, unified_score: float, correlation: Dict[str, Any]) -> str:
        """ç·åˆè©•ä¾¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆ"""
        if unified_score >= 0.9:
            level = "å„ªç§€"
        elif unified_score >= 0.75:
            level = "è‰¯å¥½"
        elif unified_score >= 0.6:
            level = "æ™®é€š"
        else:
            level = "è¦æ”¹å–„"
        
        return f"çµ±åˆè©•ä¾¡: {level} (ã‚¹ã‚³ã‚¢: {unified_score:.1%}) - {correlation['insight']}"

def main():
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ (æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ å¯¾å¿œ)"""
    print("ğŸ¯ Unified SRTA Evaluation System - Project Integration Test")
    print("=" * 60)
    
    # çµ±åˆè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    config = {
        'weights': {'responsibility': 0.6, 'quality': 0.4},
        'quality_thresholds': {
            'EXCELLENT': 0.85,
            'GOOD': 0.70,
            'FAIR': 0.55,
            'POOR': 0.40
        }
    }
    
    unified_evaluator = UnifiedSRTAEvaluationLayer(config)
    
    # æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_context = {
        'explanation_text': """
This image classification decision was executed based on predefined quality criteria.

Decision Process:
1. Dataset: ImageNet pre-trained model usage
2. Processing: ResNet-50 architecture feature extraction  
3. Criteria: Confidence threshold 87% or higher for classification confirmation

For example, detected triangular ear shapes are characteristic indicators of felines.
Responsible: AI System Administrator (ID: SYS001)
        """.strip(),
        'actor_type': 'hybrid',
        'responsible_entity': 'AI System Administrator'
    }
    
    try:
        print("\nğŸ” æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = unified_evaluator.comprehensive_evaluate(test_context)
        
        print(f"\nğŸ“Š çµ±åˆè©•ä¾¡çµæœ:")
        print(f"   çµ±åˆã‚¹ã‚³ã‚¢: {result.unified_score:.1%}")
        print(f"   ç›¸é–¢ãƒ‘ã‚¿ãƒ¼ãƒ³: {result.correlation_analysis['pattern']}")
        print(f"   å‡¦ç†æ™‚é–“: {result.processing_time:.3f}s")
        
        print(f"\nğŸ” è²¬ä»»è¿½è·¡åˆ†æ:")
        resp = result.responsibility_analysis
        print(f"   ãƒ¬ãƒ™ãƒ«: {resp['level']}")
        print(f"   ç·åˆã‚¹ã‚³ã‚¢: {resp['metrics']['overall']:.1%}")
        
        print(f"\nğŸ“ˆ å“è³ªè©•ä¾¡:")
        qual = result.quality_assessment
        print(f"   ãƒ¬ãƒ™ãƒ«: {qual['quality_level']}")
        print(f"   ç·åˆã‚¹ã‚³ã‚¢: {qual['metrics']['overall']:.1%}")
        
        print(f"\nğŸ’¡ çµ±åˆæ¨å¥¨äº‹é …:")
        for rec in result.recommendations:
            print(f"   â€¢ {rec}")
        
        print(f"\nğŸ“ ç·åˆè©•ä¾¡: {result.overall_assessment}")
        
        # æ—¢å­˜APIäº’æ›æ€§ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ”§ æ—¢å­˜APIäº’æ›æ€§ãƒ†ã‚¹ãƒˆ:")
        compat_result = unified_evaluator.evaluate_explanation(test_context)
        print(f"   evaluate_explanation() äº’æ›æ€§: âœ…")
        print(f"   è¾æ›¸ã‚¢ã‚¯ã‚»ã‚¹ result['unified_score']: {compat_result['unified_score']:.1%}")
        
        print(f"\nâœ… æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèªå®Œäº†!")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()
